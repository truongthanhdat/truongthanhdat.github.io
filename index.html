<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Thanh-Dat Truong</title>

  <meta name="author" content="Thanh-Dat Truong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Thanh-Dat Truong
                    <br>
                  </p>
                  <p>Thanh-Dat Truong is currently a PhD candidate at the University of Arkansas (UoA), where he is supervised by
                  <a href="https://uark-cviu.github.io/">Dr. Khoa Luu</a>. He received his B.Sc. degree in Computer Science from the Honors Program, University of Science, VNU in 2019 under the supervisions 
                  of <a href="https://uark-cviu.github.io/">Dr. Khoa Luu</a>  and <a hred="https://scholar.google.com/citations?user=lt2ATkkAAAAJ&hl=en">Dr. Minh-Triet Tran</a>. 
                  He was a research intern with <a href="https://minhdo.ece.illinois.edu/">Dr. Minh Do</a> at Coordinated Science Laboratory at the University of Illinois at Urbana-Champaign in 2018.
                </p>
                  
                  <p style="text-align:center">
                    <a href="mailto:tt032@uark.edu">Email</a> &nbsp;/&nbsp;
                    <!-- <a href="data/ThanhDatTruong-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/ThanhDatTruong-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=qrmxykkAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <!-- <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp; -->
                    <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                    <a href="https://twitter.com/dattruong_uark">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://github.com/truongthanhdat/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="assets/images/DatTruong.jpeg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="assets/images/DatTruong.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Recent News</h2>

                    <p>Oct. 2023: Thanh-Dat Truong was awarded NeurIPS 2023 Scholar Award for presenting his paper at NeurIPS'23. Congratulations!</p>
                    <p>Sep. 2023: One paper has been accepted to
                    <a href="https://neurips.cc/">Thirty-seventh Conference on Neural Information Processing Systems, 2023</a>. Congratulations!</p>
                    <p>Sep. 2023: Thanh-Dat Truong was awarded the Reginald R. "Barney" & Jameson A. Baxter Endowed Graduate Fellowship for the academic year 2023-2024.</p>
                    
                    <p>Apr. 2023: Thanh-Dat Truong has been awarded a Doctoral Student Presentation Travel Grant to present research at CVPR 2023.</p>											
                    
                    <p>Mar. 2023: One paper has been accepted to
                    <a
                      href="https://www.sciencedirect.com/journal/neurocomputing/">Neurocomputing</a>
                    (IF: 5.779). Congratulations!</p>
                    
                    <p>Feb. 2023: One paper has been accepted to
                    <a href="https://cvpr2023.thecvf.com/">IEEE/CVF Conference on Computer
                      Vision and Pattern Recognition, 2023</a>. Congratulations!</p>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    <!-- Thanh-Dat Truong is interested in computer vision, unsupervised domain adaptation, continual learning, and fairness -->
                    <!-- learning. -->
                    Thanh-Dat Truong research aims to develop robust and fair vision learning approach.
                    He is interested in computer vision, unsupervised domain adaptation, continual learning, and action recognition.
                    <!-- Representative papers of Thanh-Dat Truong's research are <span class="highlight">highlighted</span>. -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/FALCON.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2311.15965">
                    <span class="papertitle">FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding in Open World</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Utsav Prabhu, Bhiksha Raj, Jackson Cothren, and Khoa Luu
                  <br>
                  <em>arXiv</em>, 2023
                </td>
              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/FairCL.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2305.15700">
                    <span class="papertitle">Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Hoang-Quan Nguyen, Bhiksha Raj, and Khoa Luu
                  <br>
                  <em>Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/FREDOM.jpg" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2304.02135">
                    <span class="papertitle">FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Ngan Le, Bhiksha Raj, Jackson Cothren, and Khoa Luu
                  <br>
                  <em>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/LIAAD.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2004.05085">
                    <span class="papertitle">LIAAD: Lightweight Attentive Angular Distillation for Large-scale Age-Invariant Face Recognition</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Kha Gia Quach, Ngan Le, Tien D. Bui, and Khoa Luu
                  <br>
                  <em>Neurocomputing</em>, 2023
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/Vec2Face-V2.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2209.04920">
                    <span class="papertitle">Vec2Face-v2: Unveil Human Faces from their Blackbox Features via Attention-based Network in Face Recognition</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Ngan Le, Marios Savvides, and Khoa Luu
                  <br>
                  <em>arXiv</em>, 2023
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/CROVIA.jpeg" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="">
                    <span class="papertitle">CROVIA: Seeing Drone Scenes from Car Perspective via Cross-View Adaptation</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Ashley Dowling, Son Lam Phung, Jackson Cothren, and Khoa Luu
                  <br>
                  <em>arXiv</em>, 2023
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/CoMaL.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2304.07372">
                    <span class="papertitle">CoMaL: Conditional Maximum Likelihood Approach to Self-supervised Domain Adaptation in Long-tail Semantic Segmentation</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Pierce Helton, Ashley Dowling, Xin Li, and Khoa Luu
                  <br>
                  <em>arXiv</em>, 2023
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/CVAR.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2305.15699">
                    <span class="papertitle">Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong> and Khoa Luu
                  <br>
                  <em>arXiv</em>, 2023
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/CONDA.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2212.00621">
                    <span class="papertitle">CONDA: Continual Unsupervised Domain Adaptation Learning in Visual Perception for Self-Driving Cars</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Pierce Helton, Ahmed Moustafa, Jackson David Cothren, Khoa Luu
                  <br>
                  <em>arXiv</em>, 2022
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/CrowdCouting.gif" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9897440">
                    <span class="papertitle">Self-Supervised Domain Adaptation in Crowd Counting</span>
                  </a>
                  <br>
                  Pha Nguyen, <strong>Thanh-Dat Truong</strong>, Miaoqing Huang, Yi Liang, Ngan Le, and Khoa Luu
                  <br>
                  <em>The IEEE International Conference on Image Processing (ICIP)</em>, 2022
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/OTAdapt.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2205.10738">
                    <span class="papertitle">OTAdapt: Optimal Transport-based Approach For Unsupervised Domain Adaptation</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>,  Raviteja NVS Chappa, Xuan Bac Nguyen, Ngan Le, Ashley Dowling, and Khoa Luu
                  <br>
                  <em>The International Conference on Pattern Recognition (ICPR)</em>, 2022
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/DirecFormer.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2203.10233">
                    <span class="papertitle">DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok Seo, Son Lam Phung, Xin Li, and Khoa Luu
                  <br>
                  <em>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/BiMaL.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2108.03267">
                    <span class="papertitle">BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Ngan Le, Son Lam Phung, Chase Rainwater, and Khoa Luu
                  <br>
                  <em>The IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2021
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/Right2Talk.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2203.10233">
                    <span class="papertitle">The Right to Talk: An Audio-Visual Transformer Approach</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, The De Vu, Hoang Anh Pham, Bhiksha Raj, Ngan Le,and Khoa Luu
                  <br>
                  <em>The IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2021
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/invnxn.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2108.03267">
                    <span class="papertitle">Fast Flow Reconstruction via Robust Invertible nxn Convolution</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Minh-Triet Tran, Ngan Le, and Khoa Luu
                  <br>
                  <em>Future Internet</em>, 2021
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/DyGLIP.jpeg" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2106.06856">
                    <span class="papertitle">DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking</span>
                  </a>
                  <br>
                  Kha Gia Quach, Pha Nguyen, Huu Le, <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Minh-Triet Tran, and Khoa Luu
                  <br>
                  <em>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/Vec2Face.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2003.06958">
                    <span class="papertitle">Vec2Face: Unveil Human Faces from their Blackbox Features in Face Recognition</span>
                  </a>
                  <br>
                  Chi Nhan Duong, <strong>Thanh-Dat Truong</strong>, Kha Gia Quach, Hung Bui, Kaushik Roy, and Khoa Luu
                  <br>
                  <em>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/papers/EUNVP.png" alt="blind-date" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9108692/">
                    <span class="papertitle">Domain Generalization via Universal Non-volume Preserving Approach</span>
                  </a>
                  <br>
                  <strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Khoa Luu, Minh-Triet Tran, and Ngan Le
                  <br>
                  <em>The Conference on Computer and Robot Vision (CRV)</em>, 2020
                </td>
              </tr>

            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="assets/images/cvf.jpg" width="160"></td>
                <td width="75%" valign="center">
                  Program Committee, CVPR Precognition Workshop (since 2020)
                  <br>
                  Reviewer, CVPR
                  <br>
                  Reviewer, ICCV
                  <br>
                  Reviewer, WACV
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/images/computer-vision-logo.jpg" width="160">
                </td>
                <td width="75%" valign="center">
                  Reviewer, ECCV
                  <br>
                  Reviewer, ACCV
                  <br>
                  Reviewer, ICPR
                  <br>
                  Reviewer, Computers, Environment and Urban Systems
                </td>
              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/images/IEEE-Logo.jpg" width="160">
                </td>
                <td width="75%" valign="center">
                  Reviewer, IEEE TPAMI
                  <br>
                  Reviewer, IEEE TIP
                  <br>
                  Reviewer, IEEE TAI
                  <br>
                  Reviewer, IEEE TCSVT
                  <br>
                  Reviewer, IEEE Access
                </td>
              </tr>


              <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/images/uark-logo.png" width="160">
                </td>
                <td width="75%" valign="middle">
                  Teaching Assistant of NACME Google Applied Machine Learning Intensive Summer Bootcamp, 2021, 2022.
                  <br>
                  CSCE4613: Artificial Intelligence (Guest Lecturer).
                  <br>
                  CSCE5703: Computer Vision (Guest Lecturer).
                  <br>
                  CSCE4263: Advanced Data Structures (Guest Lecturer).
                  <br>
                  CSCE4133: Algorithms (Guest Lecturer).
                </td>
              </tr>


            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    The source code of this webpage is adopted from <a href="https://jonbarron.info/">Dr. Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>