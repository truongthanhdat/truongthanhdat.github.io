<!DOCTYPE html>
<html lang="en">
<script>
	(function (i, s, o, g, r, a, m) {
		i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
			(i[r].q = i[r].q || []).push(arguments)
		}, i[r].l = 1 * new Date(); a = s.createElement(o),
			m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
	})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

	ga('create', 'UA-37298602-1', 'auto');
	ga('send', 'pageview');

</script>

<head>
	<title>Thanh-Dat Truong</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="author" content="owwwlab.com">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<meta name="description" content="Thanh-Dat Truong" />
	<meta name="keywords" content="faculty profile, theme,css, html, jquery, transition, transform, 3d, css3" />

	<link rel="shortcut icon" href="../favicon.ico">

	<!--CSS styles-->
	<link rel="stylesheet" href="css/bootstrap.css">
	<link rel="stylesheet" href="css/font-awesome.css">
	<link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
	<link rel="stylesheet" href="css/magnific-popup.css">
	<link rel="stylesheet" href="css/style.css">
	<link id="theme-style" rel="stylesheet" href="css/styles/default.css">


	<!--/CSS styles-->
	<!--Javascript files-->
	<script type="text/javascript" src="js/jquery-1.11.3.min.js"></script>
	<script type="text/javascript" src="js/TweenMax.min.js"></script>
	<script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
	<script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>

	<script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
	<script type="text/javascript" src="js/jquery.dropdownit.js"></script>

	<script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

	<script type="text/javascript" src="js/bootstrap.min.js"></script>

	<script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

	<script type="text/javascript" src="js/masonry.min.js"></script>

	<script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
	<script type="text/javascript" src="js/jquery.nicescroll.min.js"></script>

	<script type="text/javascript" src="js/magnific-popup.js"></script>
	<script type="text/javascript" src="js/custom.js"></script>

	<!--/Javascript files-->

</head>

<body>

	<div id="wrapper">
		<a href="#sidebar" class="mobilemenu">
			<i class="fa fa-reorder"></i>
		</a>

		<div id="sidebar">
			<div id="sidebar-wrapper">
				<div id="sidebar-inner">
					<!-- Profile/logo section-->
					<div id="profile" class="clearfix">
						<div class="portrate hidden-xs">
							<img class="logo-khtn" src="img/personal/UA_Logo.png" alt="">
						</div>
						<div class="title">
							<h2>Dat T. Truong</h2>
							<h3>University of Arkansas</h3>
							<h3>Arkansas, USA</h3>
						</div>
					</div>
					<!-- /Profile/logo section-->

					<!-- Main navigation-->
					<div id="main-nav">
						<ul id="navigation">
							<li>
								<a href="index.html">
									<i class="fa fa-home"></i>
									<div class="text">Home</div>
								</a>
							</li>
							<li>
								<a href="bio.html">
									<i class="fa fa-user"></i>
									<div class="text">About Me</div>
								</a>
							</li>

							<li class="currentmenu">
								<a href="publication.html">
									<i class="fa fa-edit"></i>
									<div class="text">Publications</div>
								</a>
							</li>

							<li>
								<a href="contact.html">
									<i class="fa fa-calendar"></i>
									<div class="text">Contact Me</div>
								</a>
							</li>


						</ul>
					</div>
					<!-- /Main navigation-->
					<!-- Sidebar footer -->
					<div id="sidebar-footer">
						<div class="social-icons">
							<ul>
								<li>
									<a href="https://www.linkedin.com/in/thanh-dat-truong-098544144/">
										<i class="fa fa-linkedin"></i>
									</a>
								</li>
							</ul>
						</div>


					</div>
					<!-- /Sidebar footer -->
				</div>

			</div>
		</div>

		<div id="main">

			<div id="publications" class="page">
				<div class="page-container">
					<div class="pageheader">
						<div class="headercontent">
							<div class="section-container">
								<h2 class="title">Publications - 
								<a href="https://scholar.google.com/citations?hl=en&user=qrmxykkAAAAJ">Google Scholar</a></h2>
							</div>
						</div>
					</div>

					<div class="pagecontents">
						<!--
                            <div class="section color-1" id="filters"> <div class="section-container"> <div class="row">

                                        <div class="col-md-3"> <h3>Filter by type:</h3> </div> <div class="col-md-6"> <select id="cd-dropdown" name="cd-dropdown" class="cd-select"> 
                                        <option class="filter" value="all" selected>All types</option> 
                                        <option class="filter" value="jpaper">Journal Papers</option> 
                                        <option class="filter" value="cpaper">Conference Papers</option> 
                                        <option class="filter" value="wpaper">Workshop/Short Papers</option>
										<option class="filter" value="workingpapers">Working Papers</option>
                                        <!--<option class="filter" value="bookchapter">Book Chapters</option> <option class="filter" value="book">Books</option> <option class="filter" value="report">Reports</option>
                                        <option class="filter" value="tpaper">Technical Papers</option> -->
						</select>
					</div>
					<!--
                                        <div class="col-md-3" id="sort"> <span>Sort by year:</span> <div class="btn-group pull-right">

                                                <button type="button" data-sort="data-year" data-order="desc" class="sort btn btn-default"><i class="fa fa-sort-numeric-asc"></i></button> <button type="button" data-sort="data-year" data-order="asc" class="sort btn btn-default"><i class="fa
                                                fa-sort-numeric-desc"></i></button> </div> </div> </div> </div> </div>
-->
					<div class="section color-2" id="pub-grid">
						<div class="section-container">

							<div class="row">
								<div class="col-md-12">
									<div class="pitems">
										<h2>2021</h2>

										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Ngan Le, Son Lam Phung, Chase Rainwater, 
													and Khoa Luu
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													International Conference on Computer Vision (ICCV) 2021 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Semantic segmentation aims to predict pixel-level labels. It has become a popular task in various computer vision applications.
													 While fully supervised segmentation methods have achieved high accuracy on large-scale vision datasets, they are unable to generalize 
													 on a new testing environment or a new domain well. In this work, we first introduce a new Unaligned Domain Score to measure the efficiency 
													 of a learned model on a new target domain in unsupervised manner. Then, we present the new Bijective Maximum Likelihood (BiMaL) loss that 
													 is a generalized form of the Adversarial Entropy Minimization without any assumption about pixel independence. We have evaluated the proposed 
													 BiMaL on two domains. The proposed BiMaL approach consistently outperforms the SOTA methods on empirical experiments on
													  ``SYNTHIA to Cityscapes'', ``GTA5 to Cityscapes'', and ``SYNTHIA to Vistas''.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">The Right to Talk: An Audio-Visual Transformer Approach</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, The De Vu, Hoang Anh Pham, Bhiksha Raj, Ngan Le,
													and Khoa Luu
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													International Conference on Computer Vision (ICCV) 2021 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Turn-taking has played an essential role in structuring the regulation of a conversation. The task of identifying
													 the main speaker (who is properly taking his/her turn of speaking) and the interrupters (who are interrupting or 
													 reacting to the main speaker’s utterances) remains a challenging task. Although some prior methods have partially 
													 addressed this task, there still remain some limitations. Firstly, a direct association of Audio and Visual features may 
													 limit the correlations to be extracted due to different modalities. Secondly, the relationship across temporal segments helping 
													 to maintain the consistency of localization, separation and conversation contexts is not effectively exploited. 
													 Finally, the interactions between speakers that usually contain the tracking and anticipatory decisions about 
													 transition to a new speaker is usually ignored. Therefore, this work introduces a new Audio-Visual Transformer 
													 approach to the problem of localization and highlighting the main speaker in both audio and visual channels of a 
													 multi-speaker conversation video in the wild. The proposed method exploits different types of correlations presented 
													 in both visual and audio signals. The temporal audio-visual relationships across spatial-temporal space are anticipated and 
													 optimized via the self-attention mechanism in a Transformer structure. Moreover, a newly collected dataset is introduced for 
													 the main speaker detection. To the best of our knowledge, it is one of the first studies that is able to automatically localize 
													 and highlight the main speaker in both visual and audio channels in multi-speaker conversation videos.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Fast Flow Reconstruction via Robust Invertible n×n Convolution</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Minh-Triet Tran, Ngan Le, 
													and Khoa Luu
												</div>
												<div class="pubcite">
													<span class="label label-success">Journal Paper</span>
													Future Internet (Impact Factor: 3.64)
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Flow-based generative models have recently become one of the most efficient 
													approaches to model data generation. Indeed, they are constructed with a 
													sequence of invertible and tractable transformations. Glow first introduced 
													a simple type of generative flow using an invertible convolution. 
													However, the convolution suffers from limited flexibility compared to 
													the standard convolutions. In this paper, we propose a novel invertible 
													convolution approach that overcomes the limitations of the invertible convolution. 
													In addition, our proposed network is not only tractable and invertible but also uses 
													fewer parameters than standard convolutions. The experiments on CIFAR-10, ImageNet 
													and Celeb-HQ datasets, have shown that our invertible convolution helps to improve 
													the performance of generative models significantly
												</p>
											</div>


										</div>

										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">DyGLIP: A Dynamic Graph Model with Link Prediction
													for Accurate Multi-Camera Multiple Object Tracking</h4>
												<div class="pubauthor">Kha Gia Quach, Pha Nguyen, Huu Le,
													<strong>Thanh-Dat Truong</strong>, Chi Nhan Duong, Minh-Triet Tran
													and Khoa Luu
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													The Conference on Computer Vision and Pattern Recognition 2021
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Multi-Camera Multiple Object Tracking (MC-MOT) is an important
													computer vision
													problem due to its emerging applicability in several real-world
													applications.
													Despite a large number of existing works, solving the data
													association problem
													in any MC-MOT pipeline is arguably one of the most challenging
													tasks. Developing
													a robust MC-MOT system, however, is still extremely challenging due
													to many practical
													issues such as inconsistent lighting conditions, varying object
													movement patterns or
													the trajectory occlusions of the objects between the cameras. To
													address these problems,
													this work therefore proposes a new Dynamic Graph Model with Link
													Prediction (DyGLIP) approach
													to solve the data association task. Compared to existing methods,
													our new model
													offers several advantages, including better feature representations,
													and the ability to recover from lost tracks during camera
													transitions.
													Moreover, our model works gracefully regardless of the overlapping
													ratios between the cameras.
													Experimental results show that we outperform existing MC-MOT
													algorithms by a large margin
													on several practical datasets. Notably, our model works favorably on
													online settings,
													but can be extended to an incremental approach for large-scale
													datasets.
												</p>
											</div>


										</div>



										<h2>2020</h2>

										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">A Multi-task Contextual Atrous Residual Network for
													Brain Tumor Detection & Segmentation</h4>
												<div class="pubauthor">Ngan Le, Kashu Yamazaki, <strong>Thanh-Dat
														Truong</strong>, Kha Gia Quach and Marios Savvides

												</div>
												<div class="pubcite">
													<span class="label label-success">Conference
														Paper</span>International Conference on Pattern Recognition,
													2020
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													In recent years, deep neural networks have achieved state-of-the-art
													performance in
													a variety of recognition and segmentation tasks in medical imaging
													including brain
													tumor segmentation. We investigate that segmenting a brain tumor is
													facing to the
													imbalanced data problem where the number of pixels belonging to the
													background class
													(non tumor pixel) is much larger than the number of pixels belonging
													to the foreground
													class (tumor pixel). To address this problem, we propose a
													multi-task network which is
													formed as a cascaded structure. Our model consists of two targets,
													i.e., (i) effectively
													differentiate the brain tumor regions and (ii) estimate the brain
													tumor mask. The first
													objective is performed by our proposed contextual brain tumor
													detection network, which
													plays a role of an attention gate and focuses on the region around
													brain tumor only while
													ignoring the far neighbor background which is less correlated to the
													tumor. The second
													objective is built upon a 3D atrous residual network and under an
													encode-decode network
													in order to effectively segment both large and small objects (brain
													tumor). Our 3D atrous
													residual network is designed with a skip connection to enables the
													gradient from the deep
													layers to be directly propagated to shallow layers, thus, features
													of different depths are
													preserved and used for refining each other. In order to incorporate
													larger contextual
													information from volume MRI data, our network utilizes the 3D atrous
													convolution with
													various kernel sizes, which enlarges the receptive field of filters.
													Our proposed network
													has been evaluated on various datasets including BRATS2015,
													BRATS2017 and BRATS2018 datasets
													with both validation set and testing set. Our performance has been
													benchmarked by both
													region-based metrics and surface-based metrics. We also have
													conducted comparisons against
													state-of-the-art approaches.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Active Contour Model in Deep Learning Era: A Revise
													and Review</h4>
												<div class="pubauthor">T Hoang Ngan Le, Khoa Luu, Chi Nhan Duong, Kha
													Gia Quach, <strong>Thanh-Dat
														Truong</strong>, Kyle Sadler and Marios Savvides

												</div>
												<div class="pubcite">
													<span class="label label-success">Book Chapter</span>Applications of
													Hybrid Metaheuristic Algorithms for Image Processing
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Active Contour (AC)-based segmentation has been widely used to solve
													many
													image processing problems, specially image segmentation. While these
													AC-based
													methods offer object shape constraints, they typically look for
													strong edges
													or statistical modeling for successful segmentation.
													Clearly, AC-based approaches lack a way to work with labeled images
													in a supervised machine learning framework. Furthermore, they are
													unsupervised approaches and strongly depend on many parameters which
													are chosen by empirical results. Recently, Deep Learning (DL) has
													become the go-to method for solving many problems in various areas.
													Over the past decade, DL has achieved remarkable success in various
													artificial intelligence research areas. DL is supervised methods and
													requires large volume ground-truth. This paper first provides a
													fundamental of both Active Contour techniques and Deep Learning ...
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Domain Generalization via Universal Non-volume
													Preserving Approach</h4>
												<div class="pubauthor"><strong>Thanh-Dat Truong</strong>, Chi Nhan
													Duong, Khoa Luu, Minh-Triet Tran and Ngan Le
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													17th Conference on Robot and Computer Vision, 2020
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Recognition across domains has recently become an active topic in
													the research community.
													However, it has been largely overlooked in the problem of
													recognition in new unseen domains.
													Under this condition, the delivered deep network models are unable
													to be updated, adapted or fine-tuned.
													Therefore, recent deep learning techniques, such as domain
													adaptation, feature transferring, and fine-tuning, cannot be
													applied.
													This paper presents a novel approach to the problem of domain
													generalization in the context of deep learning.
													The proposed method is evaluated on different datasets in various
													problems, i.e. (i) digit recognition on MNIST, SVHN and MNIST-M,
													(ii) face recognition on Extended Yale-B, CMU-PIE and CMU-MPIE, and
													(iii) pedestrian recognition on RGB and Thermal image datasets.
													The experimental results show that our proposed method consistently
													improves the performance accuracy.
													It can be also easily incorporated with any other CNN frameworks
													within an end-to-end deep network design for object detection and
													recognition problems to improve their performance.
												</p>
											</div>
										</div>


										<div class="item mix cpaper" data-year="2019">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Vec2Face: Unveil Human Faces from their Blackbox
													Features in Face Recognition</h4>
												<div class="pubauthor">Chi Nhan Duong, <strong>Thanh-Dat
														Truong</strong>, Khoa Luu, Kha Gia Quach, Hung Bui, Kaushik Roy
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													The Conference on Computer Vision and Pattern Recognition 2020
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Unveiling face images of a subject given his/her high-level
													representations extracted from a blackbox Face Recognition engine is
													extremely challenging.
													It is because the limitations of accessible information from that
													engine including its structure and uninterpretable extracted
													features.
													This paper presents a novel generative structure with Bijective
													Metric Learning, namely Bijective Generative Adversarial Networks in
													a Distillation framework (DiBiGAN),
													for synthesizing faces of an identity given that person's features.
													In order to effectively address this problem, this work firstly
													introduces a bijective metric so that
													the distance measurement and metric learning process can be directly
													adopted in image domain for an image reconstruction task. Secondly,
													a distillation process is introduced
													to maximize the information exploited from the blackbox face
													recognition engine. Then a Feature-Conditional Generator Structure
													with Exponential Weighting Strategy is presented
													for a more robust generator that can synthesize realistic faces with
													ID preservation. Results on several benchmarking datasets including
													CelebA, LFW, AgeDB, CFP-FP against
													matching engines have demonstrated the effectiveness of DiBiGAN on
													both image realism and ID preservation properties.
												</p>
											</div>
										</div>


										<h2>2019</h2>

										<div class="item mix cpaper" data-year="2019">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Vehicle Re-identification with Learned
													Representation and Spatial
													Verification and Abnormality Detection with Multi-Adaptive Vehicle
													Detectors for Traffic Video Analysis</h4>
												<div class="pubauthor">Khac-Tuan Nguyen, Trung-Hieu Hoang, Minh-Triet
													Tran, Trung-Nghia Le, Ngoc-Minh Bui, Trong-Le Do, Viet-Khoa Vo-Ho,
													Quoc-An Luong, Minh-Khiem Tran, Thanh-An Nguyen, <strong>Thanh-Dat
														Truong</strong> Vinh-Tiep Nguyen and Minh N. Do
												</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													NVIDIA AI City Challenge Workshop within the Conference on Computer
													Vision and Pattern Recognition 2019
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Traffic flow analysis is essential for intelligent trans- portation
													systems.
													In this paper, we propose methods for two challenging problems in
													traffic flow analysis:
													vehicle re-identification and abnormal event detection. For the
													first problem, we propose to combine learned high-level
													features for vehicle instance representation with hand-crafted local
													features for spatial verification. For the second problem,
													we propose to use multiple adaptive vehicle detectors for anomaly
													proposal and use heuristics properties extracted from anomaly
													proposals to determine anomaly events.
													<br>
													Experiments on the datasets of traffic flow analysis from AI City
													Challenge 2019 show that our methods achieve mAP
													of 0.4008 for vehicle re-identification in Track 2, and can detect
													abnormal events with very high accuracy (F1 = 0.9429)
													in Track 3.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2019">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">HCMUS at the NTCIR-14 Lifelog-3 Task</h4>
												<div class="pubauthor">Nguyen-Khang Le, Dieu-Hien Nguyen, Trung-Hieu
													Hoang, Thanh-An Nguyen, <strong>Thanh-Dat Truong</strong>, Duy-Tung
													Dinh,
													Quoc-An Luong, Viet-Khoa Vo-Ho, Vinh-Tiep Nguyen and Minh-Triet Tran
												</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													NTCIR-14
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Lifelogging has been gaining more and more attention in the research
													community in recent years.
													Not only can it provide valuable insight and a deeper understanding
													of human daily activities,
													but it can also be used to improve personal health and wellness.
													However, there are many challenging
													problems in this field. One of the most important tasks of
													processing lifelog data is to access its semantic,
													which aims to retrieve the moments of interest from the lifelog.
													There are many approaches to this problem, two of
													which are using data processing and providing friendly user
													interaction. Our proposed system takes both of these approaches.
													We first extract concepts from the images, build a structure to
													quickly query images based on these concepts.
													We then provide users with a friendly user interface to perform the
													task.
												</p>
											</div>
										</div>


										<div class="item mix cpaper" data-year="2019">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Smart Lifelog Retrieval System with Habit-based
													Concepts and Moment Visualization</h4>
												<div class="pubauthor">Nguyen-Khang Le, Dieu-Hien Nguyen, Trung-Hieu
													Hoang, Thanh-An Nguyen, <strong>Thanh-Dat Truong</strong>, Duy-Tung
													Dinh,
													Quoc-An Luong, Viet-Khoa Vo-Ho, Vinh-Tiep Nguyen and Minh-Triet Tran
												</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													Lifelogging Search Challenge 2019
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Lifelogging is a new trend of research in which we collect and
													analyze people's daily activities to
													gain insight into people daily basis and enhance people's life
													quality. This leads to many research
													in retrieval systems to be developed for lifelogging analysis. One
													of the most important factors of
													a successful lifelogging retrieval system is to enhance the user's
													ability to perform any queries,
													even arbitrary ones. This motivates our proposal of a retrieval
													system in which we focus on the two main
													properties. The first property is the ability to accurately annotate
													the lifelog dataset with metadata that
													is meaningful for the retrieving process. The second property focus
													on a powerful user interface that supports
													a novice user to perform the retrieval efficiently.
												</p>
											</div>
										</div>

										<h2>2018</h2>

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Lifelog Moment Retrieval with Visual Concept Fusion
													and Text-based Query Expansion</h4>
												<div class="pubauthor">Minh-Triet Tran, Tung Dinh-Duy, <strong>Thanh-Dat
														Truong</strong>, Viet-Khoa Vo-Ho, Quoc-An Luong, Vinh-Tiep
													Nguyen</div>
												<div class="pubcite">
													<span class="label label-success">Working Note</span>
													CLEF 2018 Working Notes in the CEUR-WS
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Lifelog data provide potential insight analysis and understanding
													about people in their daily activities.
													However, it is still a challenging problem to index lifelog data
													efficiently and to provide a user-friendly
													interface that supports users to retrieve moments of interest. This
													motivates our proposed system to retrieve
													lifelog moment based on visual concept fusion and text-based query
													expansion. We first extract visual concepts,
													including entities, actions, and places from images. Besides
													NeuralTalk, we also proposed a novel method using concept-encoded
													feature augmentation to generate text descriptions to exploit
													further semantics of images.
													<br>
													Our proposed lifelog retrieval system allows a user to search for
													lifelog moment with four different types of
													queries on place, time, entity, and extra biometric data.
													Furthermore, the key feature of our proposed system is
													to automatically suggest concepts related to input query concepts to
													efficiently assist a user to expand a query.
													Experimental results on Lifelog moment retrieval dataset of
													ImageCLEF 2018 demonstrate the potential usage of our method
													and system to retrieve lifelog moments.
												</p>
											</div>
										</div>


										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Traffic Flow Analysis with Multiple Adaptive
													Vehicle Detectors and Velocity Estimation with Landmark-based
													Scanlines</h4>
												<div class="pubauthor">Minh-Triet Tran, Tung Dinh-Duy,
													<strong>Thanh-Dat Truong</strong>, Vinh Ton-That, Thanh-Nhon Do,
													Quoc-An Luong, Vinh-Tiep Nguyen, Minh N. Do
												</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													NVIDIA AI City Challenge Workshop within the Conference on Computer
													Vision and Pattern Recognition 2018
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													In this paper, we propose our method for vehicle detection with
													multiple adaptive vehicle detectors and velocity estimation
													with landmark-based scanlines. Inspired by the idea for tiny object
													detection, we use Faster R-CNN with Resnet-101
													to create different specialized vehicle detectors corresponding to
													different levels of details and poses. We
													propose a heuristic to check the fitness of a particular vehicle
													detector to a specific region in camera's view
													by the mean velocity direction and the mean object size. By this
													way, we can determine an adaptive set of appropriate
													vehicle detectors for each region in camera's view. Thus our system
													is expected to detect vehicles with high
													accuracy, both in precision and recall, even with tiny objects.
												</p>
												<p>
													We exploit the U.S. road rules for the length and distance of broken
													white lines on roads to propose our method for vehicle's
													velocity estimation using such landmarks. We determine
													equally-distributed scanlines, virtual parallel lines
													that are nearly-perpendicular to the road direction, with reference
													to the line connecting the corresponding
													ends of multiple broken white lines. From the timespan for a vehicle
													to cross two consecutive virtual scanlines,
													we can calculate the average vehicle's velocity within that road
													segment. We also refine the speed estimation
													by detecting when a vehicle stops at a traffic light, and smooth the
													results with a moving average filter. Experiments
													on the dataset of Traffic Flow Analysis from NVIDIA AI City
													Challenge 2018 show that our method achieves the
													perfect detect rate of 100, the average velocity difference of
													6.9762 mph on freeways, and 8.9144 mph on both
													freeways and urban roads.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Lifelogging Retrieval Based on Semantic Concepts
													Fusion</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Tung Dinh-Duy, Vinh-Tiep Nguyen,
													Minh-Triet Tran
												</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													Lifelogging Search Challenge within the International Conference on
													Multimedia Retrieval 2018 - ICMR
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Lifelogging data provides useful insight understanding about our
													lives during daily activities. Thus, it is essential to
													develop a system to assist users to retrieve events or memories from
													lifelogging data from ad-hoc text queries.
													In this paper, we first propose a method to process lifelogging data
													by grouping images into visual shots and
													clusters, then extract semantic concepts on scene category and
													attributes, entities, and actions. We then develop
													a query system that supports 4 main types of query conditions:
													temporal, spatial, entity and action, and extra
													data criteria. Our system is expected to efficiently assist users to
													search for past moments in daily logs.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Lightweight Deep Convolutional Neural Network for
													Tiny Object Recognition</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Vinh-Tiep Nguyen, Minh-Triet Tran
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													Insights Discovery from Lifelog Data (INDEED) within the 7th
													International Conference on Pattern Recognition Applications
													and Methods - ICPRAM
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Object recognition is an important problem in Computer Vision with
													many applications such as image search, autonomous car,
													image understanding, etc. In recent years, Convolutional Neural
													Network (CNN) based models have achieved great
													success on object recognition, especially VGG, ResNet, Wide ResNet,
													etc. However, these models involve a large
													number of parameters that should be trained with large-scale
													datasets on powerful computing systems. Thus, it
													is not appropriate to train a heavy CNN with small-scale datasets
													with only thousands of samples as it is easy
													to be over-fitted. Furthermore, it is not efficient to use an
													existing heavy CNN method to recognize small images,
													such as in CIFAR-10 or CIFAR-100. In this paper, we propose a
													Lightweight Deep Convolutional Neural Network
													architecture for tiny images codenamed "DCTI" to reduce
													significantly a number of parameters for such datasets.
													Additionally, we use batch-normalization to deal with the change in
													distribution each layer. To demonstrate
													the efficiency of the proposed method, we conduct experiments on two
													popular datasets: CIFAR-10 and CIFAR-100.
													The results show that the proposed network not only significantly
													reduces the number of parameters but also
													improves the performance. The number of parameters in our method is
													only 21.33% the number of parameters of
													Wide ResNet but our method achieves up to 94.34% accuracy on
													CIFAR-10, comparing to 96.11% of Wide ResNet. Besides,
													our method also achieves the accuracy of 73.65% on CIFAR-100.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Video Search Based on Semantic Extraction and
													Locally Regional Object Proposal</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Vinh-Tiep Nguyen, Minh-Triet
													Tran, Tien Do, Trang-Vinh Trieu, Duc Thanh Ngo and Dinh-Duy
													Le
												</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													Video Browser Showdown - The 24th International Conference on
													MultiMedia Modeling
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													In this paper, we propose a semantic concept based video browsing
													system which mainly focuses on the spatial information
													of concepts. A locally regional object proposal is provided to
													softly assign the object location into cells
													of a grid. For action concepts, we also collect a dataset which
													contains about 100 actions. In many cases, actions
													could be predicted from a static image, not necessarily from a video
													shot. Therefore, we treat actions like
													object concepts and propose to use a deep neural network based on
													YOLO detector for action detection. Moreover,
													instead of densely extracting concepts of a video shot, we focus on
													high saliency objects and remove noisy concepts.
													To further improve the interaction, we provide a color based sketch
													board for quickly removing irrelevant shots
													and instance search panel to improve the recall of the system. Last
													but not least, metadata such as title, summary
													are integrated into the system in order to improve the precision and
													recall.
												</p>
											</div>
										</div>



									</div>
								</div>
							</div>
						</div>

					</div>
				</div>

			</div>
		</div>
	</div>


	</div>
	</div>
</body>

</html>