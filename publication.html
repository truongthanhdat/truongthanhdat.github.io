<!DOCTYPE html>
<html lang="en">
<script>
	(function (i, s, o, g, r, a, m) {
	i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
		(i[r].q = i[r].q || []).push(arguments)
	}, i[r].l = 1 * new Date(); a = s.createElement(o),
		m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
	})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

	ga('create', 'UA-37298602-1', 'auto');
	ga('send', 'pageview');

</script>

<head>
	<title>Thanh-Dat Truong</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="author" content="owwwlab.com">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<meta name="description" content="Thanh-Dat Truong" />
	<meta name="keywords" content="faculty profile, theme,css, html, jquery, transition, transform, 3d, css3" />

	<link rel="shortcut icon" href="../favicon.ico">

	<!--CSS styles-->
	<link rel="stylesheet" href="css/bootstrap.css">
	<link rel="stylesheet" href="css/font-awesome.css">
	<link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
	<link rel="stylesheet" href="css/magnific-popup.css">
	<link rel="stylesheet" href="css/style.css">
	<link id="theme-style" rel="stylesheet" href="css/styles/default.css">


	<!--/CSS styles-->
	<!--Javascript files-->
	<script type="text/javascript" src="js/jquery-1.11.3.min.js"></script>
	<script type="text/javascript" src="js/TweenMax.min.js"></script>
	<script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
	<script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>

	<script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
	<script type="text/javascript" src="js/jquery.dropdownit.js"></script>

	<script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

	<script type="text/javascript" src="js/bootstrap.min.js"></script>

	<script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

	<script type="text/javascript" src="js/masonry.min.js"></script>

	<script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
	<script type="text/javascript" src="js/jquery.nicescroll.min.js"></script>

	<script type="text/javascript" src="js/magnific-popup.js"></script>
	<script type="text/javascript" src="js/custom.js"></script>

	<!--/Javascript files-->

</head>

<body>

	<div id="wrapper">
		<a href="#sidebar" class="mobilemenu">
			<i class="fa fa-reorder"></i>
		</a>

		<div id="sidebar">
			<div id="sidebar-wrapper">
				<div id="sidebar-inner">
					<!-- Profile/logo section-->
					<div id="profile" class="clearfix">
						<div class="portrate hidden-xs">
							<img class="logo-khtn" src="img/personal/logo-khtn.png" alt="">
						</div>
						<div class="title">
							<h2>Thanh-Dat Truong</h2>
							<h3>University of Science</h3>
							<h3>Vietnam National University</h3>
							<h3>Ho Chi Minh City</h3>
						</div>
					</div>
					<!-- /Profile/logo section-->

					<!-- Main navigation-->
					<div id="main-nav">
						<ul id="navigation">
							<li>
								<a href="index.html">
									<i class="fa fa-home"></i>
									<div class="text">Home</div>
								</a>
							</li>
							<li>
								<a href="bio.html">
									<i class="fa fa-user"></i>
									<div class="text">About Me</div>
								</a>
							</li>
							<li>
								<a href="research.html">
									<i class="fa fa-book"></i>
									<div class="text">Research</div>
								</a>
							</li>

							<li class="currentmenu">
								<a href="publication.html">
									<i class="fa fa-edit"></i>
									<div class="text">Publications</div>
								</a>
							</li>
							<li>
								<a href="project.html">
									<i class="fa fa-file-code-o"></i>
									<div class="text">Projects</div>
								</a>
							</li>
							<li>
								<a href="contact.html">
									<i class="fa fa-calendar"></i>
									<div class="text">Contact Me</div>
								</a>
							</li>


						</ul>
					</div>
					<!-- /Main navigation-->
					<!-- Sidebar footer -->
					<div id="sidebar-footer">
						<div class="social-icons">
							<ul>
								<li>
									<a href="https://www.linkedin.com/in/thanh-dat-truong-098544144/">
										<i class="fa fa-linkedin"></i>
									</a>
								</li>
							</ul>
						</div>


					</div>
					<!-- /Sidebar footer -->
				</div>

			</div>
		</div>

		<div id="main">

			<div id="publications" class="page">
				<div class="page-container">
					<div class="pageheader">
						<div class="headercontent">
							<div class="section-container">
								<h2 class="title">Publications</h2>
							</div>
						</div>
					</div>

					<div class="pagecontents">
						<!--
                            <div class="section color-1" id="filters"> <div class="section-container"> <div class="row">

                                        <div class="col-md-3"> <h3>Filter by type:</h3> </div> <div class="col-md-6"> <select id="cd-dropdown" name="cd-dropdown" class="cd-select"> 
                                        <option class="filter" value="all" selected>All types</option> 
                                        <option class="filter" value="jpaper">Journal Papers</option> 
                                        <option class="filter" value="cpaper">Conference Papers</option> 
                                        <option class="filter" value="wpaper">Workshop/Short Papers</option>
										<option class="filter" value="workingpapers">Working Papers</option>
                                        <!--<option class="filter" value="bookchapter">Book Chapters</option> <option class="filter" value="book">Books</option> <option class="filter" value="report">Reports</option>
                                        <option class="filter" value="tpaper">Technical Papers</option> -->
						</select>
					</div>
					<!--
                                        <div class="col-md-3" id="sort"> <span>Sort by year:</span> <div class="btn-group pull-right">

                                                <button type="button" data-sort="data-year" data-order="desc" class="sort btn btn-default"><i class="fa fa-sort-numeric-asc"></i></button> <button type="button" data-sort="data-year" data-order="asc" class="sort btn btn-default"><i class="fa
                                                fa-sort-numeric-desc"></i></button> </div> </div> </div> </div> </div>
-->
					<div class="section color-2" id="pub-grid">
						<div class="section-container">

							<div class="row">
								<div class="col-md-12">
									<div class="pitems">
										<h2>2019</h2>

										<div class="item mix cpaper" data-year="2019">
												<div class="pubmain">
													<div class="pubassets">
														<a href="#" class="pubcollapse">
															<i class="fa fa-expand"></i>
														</a>
													</div>
													<h4 class="pubtitle">Vehicle Re-identification with Learned Representation and Spatial 
														Verification and Abnormality Detection with Multi-Adaptive Vehicle Detectors for Traffic Video Analysis</h4>
													<div class="pubauthor">Khac-Tuan Nguyen, Trung-Hieu Hoang, Minh-Triet Tran, Trung-Nghia Le, Ngoc-Minh Bui, Trong-Le Do, Viet-Khoa Vo-Ho,
																Quoc-An Luong, Minh-Khiem Tran, Thanh-An Nguyen, <strong>Thanh-Dat Truong</strong> Vinh-Tiep Nguyen and Minh N. Do
													</div>
													<div class="pubcite">
														<span class="label label-success">Workshop Paper</span>
														NVIDIA AI City Challenge Workshop within the Conference on Computer Vision and Pattern Recognition 2019
													</div>
												</div>
												<div class="pubdetails">
													<h4>Abstract</h4>
													<p>
															Traffic flow analysis is essential for intelligent trans- portation systems.
															 In this paper, we propose methods for two challenging problems in traffic flow analysis: 
															 vehicle re-identification and abnormal event detection. For the first problem, we propose to combine learned high-level 
															 features for vehicle instance representation with hand-crafted local features for spatial verification. For the second problem, 
															 we propose to use multiple adaptive vehicle detectors for anomaly proposal and use heuristics properties extracted from anomaly 
															 proposals to determine anomaly events.
															 <br>
															Experiments on the datasets of traffic flow analysis from AI City Challenge 2019 show that our methods achieve mAP
															 of 0.4008 for vehicle re-identification in Track 2, and can detect abnormal events with very high accuracy (F1 = 0.9429) 
															 in Track 3.
													</p>
												</div>
											</div>

										<div class="item mix cpaper" data-year="2019">
												<div class="pubmain">
													<div class="pubassets">
														<a href="#" class="pubcollapse">
															<i class="fa fa-expand"></i>
														</a>
													</div>
													<h4 class="pubtitle">HCMUS at the NTCIR-14 Lifelog-3 Task</h4>
													<div class="pubauthor">Nguyen-Khang Le, Dieu-Hien Nguyen, Trung-Hieu Hoang, Thanh-An Nguyen, <strong>Thanh-Dat Truong</strong>, Duy-Tung Dinh, 
																				Quoc-An Luong, Viet-Khoa Vo-Ho, Vinh-Tiep Nguyen and Minh-Triet Tran</div>
													<div class="pubcite">
														<span class="label label-success">Workshop Paper</span>
														NTCIR-14
													</div>
												</div>
												<div class="pubdetails">
													<h4>Abstract</h4>
													<p>
															Lifelogging has been gaining more and more attention in the research community in recent years. 
															Not only can it provide valuable insight and a deeper understanding of human daily activities, 
															but it can also be used to improve personal health and wellness. However, there are many challenging
															 problems in this field. One of the most important tasks of processing lifelog data is to access its semantic,
															  which aims to retrieve the moments of interest from the lifelog. There are many approaches to this problem, two of 
															  which are using data processing and providing friendly user interaction. Our proposed system takes both of these approaches.
															   We first extract concepts from the images, build a structure to quickly query images based on these concepts.   
															   We then provide users with a friendly user interface to perform the task.
													</p>
												</div>
											</div>
											

										<div class="item mix cpaper" data-year="2019">
												<div class="pubmain">
													<div class="pubassets">
														<a href="#" class="pubcollapse">
															<i class="fa fa-expand"></i>
														</a>
													</div>
													<h4 class="pubtitle">Smart Lifelog Retrieval System with Habit-based Concepts and Moment Visualization</h4>
													<div class="pubauthor">Nguyen-Khang Le, Dieu-Hien Nguyen, Trung-Hieu Hoang, Thanh-An Nguyen, <strong>Thanh-Dat Truong</strong>, Duy-Tung Dinh, 
																				Quoc-An Luong, Viet-Khoa Vo-Ho, Vinh-Tiep Nguyen and Minh-Triet Tran</div>
													<div class="pubcite">
														<span class="label label-success">Workshop Paper</span>
														Lifelogging Search Challenge 2019
													</div>
												</div>
												<div class="pubdetails">
													<h4>Abstract</h4>
													<p>
															Lifelogging is a new trend of research in which we collect and analyze people's daily activities to 
															gain insight into people daily basis and enhance people's life quality. This leads to many research 
															in retrieval systems to be developed for lifelogging analysis. One of the most important factors of 
															a successful lifelogging retrieval system is to enhance the user's ability to perform any queries, 
															even arbitrary ones. This motivates our proposal of a retrieval system in which we focus on the two main 
															properties. The first property is the ability to accurately annotate the lifelog dataset with metadata that 
															is meaningful for the retrieving process. The second property focus on a powerful user interface that supports 
															a novice user to perform the retrieval efficiently.
													</p>
												</div>
											</div>
											
											<h2>2018</h2>

										<div class="item mix cpaper" data-year="2018">
												<div class="pubmain">
													<div class="pubassets">
														<a href="#" class="pubcollapse">
															<i class="fa fa-expand"></i>
														</a>
													</div>
													<h4 class="pubtitle">Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion</h4>
													<div class="pubauthor">Minh-Triet Tran, Tung Dinh-Duy, <strong>Thanh-Dat Truong</strong>, Viet-Khoa Vo-Ho, Quoc-An Luong, Vinh-Tiep Nguyen</div>
													<div class="pubcite">
														<span class="label label-success">Working Note</span>
														CLEF 2018 Working Notes in the CEUR-WS
													</div>
												</div>
												<div class="pubdetails">
													<h4>Abstract</h4>
													<p>
															Lifelog data provide potential insight analysis and understanding about people in their daily activities. 
															However, it is still a challenging problem to  index lifelog data efficiently and to provide a user-friendly 
															interface that supports users to retrieve moments of interest. This motivates our proposed system to retrieve 
															lifelog moment based on visual concept fusion and text-based query expansion. We first extract visual concepts, 
															including entities, actions, and places from images. Besides NeuralTalk, we also proposed a novel method using concept-encoded 
															feature augmentation to generate text descriptions to exploit further semantics of images. 
<br>
															Our proposed lifelog retrieval system allows a user to search for lifelog moment with four different types of 
															queries on place, time, entity, and extra biometric data. Furthermore, the key feature of our proposed system is 
															to automatically suggest concepts related to input query concepts to efficiently assist a user to expand a query. 
															Experimental results on Lifelog moment retrieval dataset of ImageCLEF 2018 demonstrate the potential usage of our method 
															and system to retrieve lifelog moments.
													</p>
												</div>
											</div>
	

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Traffic Flow Analysis with Multiple Adaptive Vehicle Detectors and Velocity Estimation with Landmark-based Scanlines</h4>
												<div class="pubauthor">Minh-Triet Tran, Tung Dinh-Duy,
													<strong>Thanh-Dat Truong</strong>, Vinh Ton-That, Thanh-Nhon Do, Quoc-An Luong, Vinh-Tiep Nguyen, Minh N. Do</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													NVIDIA AI City Challenge Workshop within the Conference on Computer Vision and Pattern Recognition 2018
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													In this paper, we propose our method for vehicle detection with multiple adaptive vehicle detectors and velocity estimation
													with landmark-based scanlines. Inspired by the idea for tiny object detection, we use Faster R-CNN with Resnet-101
													to create different specialized vehicle detectors corresponding to different levels of details and poses. We
													propose a heuristic to check the fitness of a particular vehicle detector to a specific region in camera's view
													by the mean velocity direction and the mean object size. By this way, we can determine an adaptive set of appropriate
													vehicle detectors for each region in camera's view. Thus our system is expected to detect vehicles with high
													accuracy, both in precision and recall, even with tiny objects.
												</p>
												<p>
													We exploit the U.S. road rules for the length and distance of broken white lines on roads to propose our method for vehicle's
													velocity estimation using such landmarks. We determine equally-distributed scanlines, virtual parallel lines
													that are nearly-perpendicular to the road direction, with reference to the line connecting the corresponding
													ends of multiple broken white lines. From the timespan for a vehicle to cross two consecutive virtual scanlines,
													we can calculate the average vehicle's velocity within that road segment. We also refine the speed estimation
													by detecting when a vehicle stops at a traffic light, and smooth the results with a moving average filter. Experiments
													on the dataset of Traffic Flow Analysis from NVIDIA AI City Challenge 2018 show that our method achieves the
													perfect detect rate of 100, the average velocity difference of 6.9762 mph on freeways, and 8.9144 mph on both
													freeways and urban roads.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Lifelogging Retrieval Based on Semantic Concepts Fusion</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Tung Dinh-Duy, Vinh-Tiep Nguyen, Minh-Triet Tran</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													Lifelogging Search Challenge within the International Conference on Multimedia Retrieval 2018 - ICMR
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Lifelogging data provides useful insight understanding about our lives during daily activities. Thus, it is essential to
													develop a system to assist users to retrieve events or memories from lifelogging data from ad-hoc text queries.
													In this paper, we first propose a method to process lifelogging data by grouping images into visual shots and
													clusters, then extract semantic concepts on scene category and attributes, entities, and actions. We then develop
													a query system that supports 4 main types of query conditions: temporal, spatial, entity and action, and extra
													data criteria. Our system is expected to efficiently assist users to search for past moments in daily logs.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Lightweight Deep Convolutional Neural Network for Tiny Object Recognition</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Vinh-Tiep Nguyen, Minh-Triet Tran</div>
												<div class="pubcite">
													<span class="label label-success">Conference</span>
													Insights Discovery from Lifelog Data (INDEED) within the 7th International Conference on Pattern Recognition Applications
													and Methods - ICPRAM
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Object recognition is an important problem in Computer Vision with many applications such as image search, autonomous car,
													image understanding, etc. In recent years, Convolutional Neural Network (CNN) based models have achieved great
													success on object recognition, especially VGG, ResNet, Wide ResNet, etc. However, these models involve a large
													number of parameters that should be trained with large-scale datasets on powerful computing systems. Thus, it
													is not appropriate to train a heavy CNN with small-scale datasets with only thousands of samples as it is easy
													to be over-fitted. Furthermore, it is not efficient to use an existing heavy CNN method to recognize small images,
													such as in CIFAR-10 or CIFAR-100. In this paper, we propose a Lightweight Deep Convolutional Neural Network
													architecture for tiny images codenamed "DCTI" to reduce significantly a number of parameters for such datasets.
													Additionally, we use batch-normalization to deal with the change in distribution each layer. To demonstrate
													the efficiency of the proposed method, we conduct experiments on two popular datasets: CIFAR-10 and CIFAR-100.
													The results show that the proposed network not only significantly reduces the number of parameters but also
													improves the performance. The number of parameters in our method is only 21.33% the number of parameters of
													Wide ResNet but our method achieves up to 94.34% accuracy on CIFAR-10, comparing to 96.11% of Wide ResNet. Besides,
													our method also achieves the accuracy of 73.65% on CIFAR-100.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Video Search Based on Semantic Extraction and Locally Regional Object Proposal</h4>
												<div class="pubauthor">
													<strong>Thanh-Dat Truong</strong>, Vinh-Tiep Nguyen, Minh-Triet Tran, Tien Do, Trang-Vinh Trieu, Duc Thanh Ngo and Dinh-Duy
													Le</div>
												<div class="pubcite">
													<span class="label label-success">Workshop Paper</span>
													Video Browser Showdown - The 24th International Conference on MultiMedia Modeling
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													In this paper, we propose a semantic concept based video browsing system which mainly focuses on the spatial information
													of concepts. A locally regional object proposal is provided to softly assign the object location into cells
													of a grid. For action concepts, we also collect a dataset which contains about 100 actions. In many cases, actions
													could be predicted from a static image, not necessarily from a video shot. Therefore, we treat actions like
													object concepts and propose to use a deep neural network based on YOLO detector for action detection. Moreover,
													instead of densely extracting concepts of a video shot, we focus on high saliency objects and remove noisy concepts.
													To further improve the interaction, we provide a color based sketch board for quickly removing irrelevant shots
													and instance search panel to improve the recall of the system. Last but not least, metadata such as title, summary
													are integrated into the system in order to improve the precision and recall.
												</p>
											</div>
										</div>



									</div>
								</div>
							</div>
						</div>

					</div>
				</div>

			</div>
		</div>
	</div>


	</div>
	</div>
</body>

</html>